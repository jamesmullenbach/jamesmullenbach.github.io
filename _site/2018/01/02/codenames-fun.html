<!doctype html>
<html lang="en-US">
  <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="chrome=1">
      <title>jamesmullenbach.github.io by jamesmullenbach</title>

      <link rel="stylesheet" href="/assets/css/style.css?v=e98adbc6fbe5c8b2525aeab2d6cda944899b9e7f">
      <meta name="viewport" content="width=device-width">
      <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <![endif]-->
    </head>
    <body>
      <div class="wrapper">
        <header>
            <div>
                <!-- <img src="/portrait.jpg" id="profpic" width=50% />
-->
          <h2> James Mullenbach </h2>
          <p></p>
          <table>
              <tr><td><a href="/">Home</a></td></tr>
              <tr><td><a href="/blog">Blog</a></td></tr>
              <tr><td><a href="/assets/cv.pdf">CV</a> (as of June '18)</td></tr>
              <tr><td></td></tr>
              <tr><td><a href="https://github.com/jamesmullenbach">GitHub</a></td></tr>
              <tr><td><a href="https://linkedin.com/in/jamesmullenbach">LinkedIn</a></td></tr>
              <tr><td><a href="https://twitter.com/jmullenbach_">Twitter</a></td></tr>
          </table>
            <!--<ul>
                <li><a href="https://github.com/jamesmullenbach">GitHub</a></li>
                <li><a href="">LinkedIn</a></li>
                <li><a href="">Blog</a></li>
            </ul>-->
          </div>

          
        </header>
        <section>

        <h1>Word vectors are (not) magic - a holiday experiment with Codenames</h1>
<p class='meta'>02 Jan 2018</p>

<div class='post'>
    <p>(This post has an accompanying Jupyter notebook that you can play around with <a href="https://github.com/jamesmullenbach/codenames">here</a>).</p>

<p>Over the break between semesters, I’ve spent a lot of time with family playing a popular board game called <a href="https://czechgames.com/en/codenames/">Codenames</a>.</p>

<p>If you haven’t played, the gist is that one player from each team, the ‘spymaster’, tries to get their team members to select their team’s assigned words from a group of 25 while avoiding the other team’s words and a game-ending ‘assassin’ word, using one word clues. It’s like the game show Password, except clues can apply to any number of words.</p>

<p>It’s a fun language based game and makes for an interesting testbed for simple experiments like the one I’m about to talk about.</p>

<p>Naturally, I thought about how a computer might play this game. Let’s look at a simple way to suggest clues for the spymaster!</p>

<h2 id="spymaster-as-a-machine-learner">Spymaster as a machine learner</h2>

<p>First, we’ll need some semantic representation for each word. For this, I chose Google’s <a href="https://code.google.com/archive/p/word2vec/">pre-trained word2vec vectors</a>, but you could use the word vector or other representation of your choosing.</p>

<p>The problem for the spymaster is a really tidy analogy for classification! Specifically, we have a dataset of 25 words, and on any turn we pick a few that we want our team to guess, while avoiding the other team’s words. We can think of our words as positive examples, and the other team’s words and the assassin word as negative examples. This plays right into the word analogy strengths of word2vec. A simple visual of an ideal clue:</p>

<p><img src="/assets/codenames.png" /></p>

<h2 id="an-example-board">An example board</h2>

<p>As an initial check that this can work, let’s generate some clues that match to just one of our team’s words. We’ll get an example board with <a href="https://horsepaste.com">this resource</a>.</p>

<p><img src="/assets/board.png" /></p>

<p>Let’s play blue, then our opponents are red. There are some ‘bystander’ words (the black ones) which belong to neither team but are helpful to avoid. Let’s ignore these for simplicity. Time to get coding! First, load the word2vec vectors into <code class="highlighter-rouge">gensim</code> (I limit the vocab for memory reasons, and to avoid esoteric clues).  We can also make lists for the relevant words.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gensim</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s">'GoogleNews-vectors-negative300.bin'</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">200000</span><span class="p">)</span>
<span class="n">ours</span> <span class="o">=</span> <span class="p">[</span><span class="s">'watch'</span><span class="p">,</span> <span class="s">'star'</span><span class="p">,</span> <span class="s">'mole'</span><span class="p">,</span> <span class="s">'Berlin'</span><span class="p">,</span> <span class="s">'limousine'</span><span class="p">,</span> <span class="s">'day'</span><span class="p">,</span> <span class="s">'wind'</span><span class="p">,</span> <span class="s">'cap'</span><span class="p">,</span> <span class="s">'thumb'</span><span class="p">]</span>                                  
<span class="n">theirs</span> <span class="o">=</span> <span class="p">[</span><span class="s">'smuggler'</span><span class="p">,</span> <span class="s">'crown'</span><span class="p">,</span> <span class="s">'cotton'</span><span class="p">,</span> <span class="s">'palm'</span><span class="p">,</span> <span class="s">'pumpkin'</span><span class="p">,</span> <span class="s">'giant'</span><span class="p">,</span> <span class="s">'link'</span><span class="p">,</span> <span class="s">'dog'</span><span class="p">]</span>                                     
<span class="n">assassin</span> <span class="o">=</span> <span class="s">'tie'</span>
</code></pre>
</div>

<h2 id="proof-of-concept-one--and-two-word-clues">Proof of concept: one- and two-word clues</h2>

<p>There’s several ways to get good one-word clues. Let’s start by just finding the most similar words to each of our team’s words:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">clues</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">our_word</span> <span class="ow">in</span> <span class="n">ours</span><span class="p">:</span>
    <span class="c">#get similar words from google vocab, and lowercase them</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">our_word</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">50</span><span class="p">)]</span>
    <span class="c">#per game rules, we should exclude multi-word results and words that use part of the clue word.</span>
    <span class="c">#we could use stemmers for this maybe, but let's keep it simple</span>
    <span class="n">clues</span><span class="p">[</span><span class="n">our_word</span><span class="p">]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="p">,</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="n">candidates</span> <span class="k">if</span> <span class="s">'_'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">our_word</span> <span class="ow">and</span> <span class="n">our_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">c</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>
</code></pre>
</div>

<p>This gives (top result only for brevity):</p>
<div class="highlighter-rouge"><pre class="highlight"><code><span class="p">{</span><span class="err">'Berlin':</span><span class="w"> </span><span class="err">('munich',</span><span class="w"> </span><span class="err">0.6743212938308716),</span><span class="w">
 </span><span class="err">'cap':</span><span class="w"> </span><span class="err">('hat',</span><span class="w"> </span><span class="err">0.4268711507320404),</span><span class="w">
 </span><span class="err">'day':</span><span class="w"> </span><span class="err">('week',</span><span class="w"> </span><span class="err">0.65529865026474),</span><span class="w">
 </span><span class="err">'limousine':</span><span class="w"> </span><span class="err">('limos',</span><span class="w"> </span><span class="err">0.6517890095710754),</span><span class="w">
 </span><span class="err">'mole':</span><span class="w"> </span><span class="err">('birthmark',</span><span class="w"> </span><span class="err">0.46605658531188965),</span><span class="w">
 </span><span class="err">'star':</span><span class="w"> </span><span class="err">('heartthrob',</span><span class="w"> </span><span class="err">0.543801486492157),</span><span class="w">
 </span><span class="err">'thumb':</span><span class="w"> </span><span class="err">('pinkie',</span><span class="w"> </span><span class="err">0.6484930515289307),</span><span class="w">
 </span><span class="err">'watch':</span><span class="w"> </span><span class="err">('see',</span><span class="w"> </span><span class="err">0.5326846837997437),</span><span class="w">
 </span><span class="err">'wind':</span><span class="w"> </span><span class="err">('gusts',</span><span class="w"> </span><span class="err">0.5962637662887573)</span><span class="p">}</span><span class="w">
</span></code></pre>
</div>

<p>Most of these seem pretty good! I’m not sure if <code class="highlighter-rouge">'limos'</code> would fly by the rules, but the next best result, <code class="highlighter-rouge">'chauffered'</code>, is valid. It’s also interesting to observe how the words with more senses <code class="highlighter-rouge">('cap', 'mole', 'watch')</code> have lower scores. Let’s try doing the same for two-word clues. First, let’s generalize the rule-keeping a bit:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">word_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="s">'_'</span> <span class="ow">in</span> <span class="n">candidate</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">candidate</span> <span class="ow">or</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
</code></pre>
</div>

<p>Now, get candidate clues for all combinations of two words with <code class="highlighter-rouge">itertools</code>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">itertools</span>
<span class="n">clues_2</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">our1</span><span class="p">,</span> <span class="n">our2</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">ours</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">our_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">our1</span><span class="p">,</span> <span class="n">our2</span><span class="p">]</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="n">our_list</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">50</span><span class="p">)]</span>
    <span class="n">clues_2</span><span class="p">[(</span><span class="n">our1</span><span class="p">,</span> <span class="n">our2</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="p">,</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="n">candidates</span> <span class="k">if</span> <span class="n">verify</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">our_list</span><span class="p">)][:</span><span class="mi">5</span><span class="p">]</span>
</code></pre>
</div>

<p>I won’t post the whole results here. Some interesting suggestions are: <code class="highlighter-rouge">'standout'</code> and <code class="highlighter-rouge">'legend'</code> for <code class="highlighter-rouge">('watch', 'star')</code>, <code class="highlighter-rouge">'spies'</code> for <code class="highlighter-rouge">('watch', 'mole')</code>, and <code class="highlighter-rouge">'weather'</code> for <code class="highlighter-rouge">('watch', 'wind')</code>. One impressive result is <code class="highlighter-rouge">'stasi'</code> for <code class="highlighter-rouge">('mole', 'Berlin')</code>, which – I had to Wikipedia this – is the <a href="https://en.wikipedia.org/wiki/Stasi">former German secret police during the Cold War</a>. A couple good clues (at least, I think so) that it misses are <code class="highlighter-rouge">'solar'</code> for <code class="highlighter-rouge">('star', 'wind')</code> and <code class="highlighter-rouge">'celebrity'</code> for <code class="highlighter-rouge">('star', 'limousine')</code>. Overall, many suggested clues really only pertain to one of the two words. So, we can improve.</p>

<h2 id="avoiding-opponents-words">Avoiding opponent’s words</h2>

<p>So far, we have not tried to avoid the ‘bad’ words: the other team’s words and the assassin. For instance, <code class="highlighter-rouge">'wrist'</code> is not a <em>great</em> clue for <code class="highlighter-rouge">('watch', 'thumb')</code>, as it may confuse teammates with the opposing team’s word <code class="highlighter-rouge">'palm'</code>. All we have to do is add the argument <code class="highlighter-rouge">negative=theirs+[assassin]</code> to the penultimate line of the last code block. Unfortunately, these results are poor and seem to be an artifact of <code class="highlighter-rouge">gensim</code>’s similarity computing method - the top result for many pairs of words is <code class="highlighter-rouge">'portfolios'</code>. I suspect that the method places too much emphasis on the negative words, as there are more negative than positive words.</p>

<p>To mitigate this, we can just use our entire team’s words as the positive examples.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">candidates</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="n">ours</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="n">theirs</span><span class="o">+</span><span class="p">[</span><span class="n">assassin</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre>
</div>

<p>Then, since these clues are really trying to capture all nine of our words, we can filter down the results and find out which of our team’s words each clue is best for.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">operator</span>
<span class="n">cand_words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">cand</span><span class="p">,</span><span class="n">_</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">verify</span><span class="p">(</span><span class="n">cand</span><span class="p">,</span> <span class="n">ours</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">our_word</span> <span class="ow">in</span> <span class="n">ours</span><span class="p">:</span>
            <span class="n">scores</span><span class="p">[</span><span class="n">our_word</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">cand</span><span class="p">,</span> <span class="n">our_word</span><span class="p">)</span>
        <span class="n">cand_words</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cand</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]))</span>
</code></pre>
</div>

<p>This gives a few promising results that can target more than one of our teams words. Here are the 10 best clues, in order, and 5 corresponding team words, that a computer thinks we should use:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[{'flashbulbs': ['star', 'limousine', 'wind', 'watch', 'day']},
 {'eve': ['day', 'star', 'Berlin', 'watch', 'limousine']},
 {'rookies': ['star', 'cap', 'day', 'watch', 'wind']},
 {'VIPs': ['limousine', 'watch', 'star', 'mole', 'Berlin']},
 {'Tomczyk': ['Berlin', 'cap', 'wind', 'watch', 'day']},
 {'Stasi': ['Berlin', 'mole', 'wind', 'watch', 'thumb']},
 {'countdown': ['day', 'watch', 'mole', 'star', 'wind']},
 {'##/#-hour': ['day', 'watch', 'limousine', 'wind', 'thumb']},
 {'invitees': ['limousine', 'star', 'watch', 'day', 'Berlin']},
 {'hour': ['day', 'watch', 'limousine', 'wind', 'star']}]
</code></pre>
</div>

<p><code class="highlighter-rouge">'Flashbulbs'</code> really seems great, though <code class="highlighter-rouge">'paparazzi'</code> may be even more direct for <code class="highlighter-rouge">('star', 'limousine', 'watch')</code>. I also like the suggestion <code class="highlighter-rouge">'night'</code> (ranked 15), for <code class="highlighter-rouge">('day', 'watch', 'star')</code>.</p>

<h2 id="clue-giving-by-a-classifier">Clue giving by a classifier</h2>

<p>Let’s try a whole new approach based on machine learning. We can easily find a linear classifier as illustrated before, then just project that to our vocabulary!</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ours</span> <span class="o">+</span> <span class="n">theirs</span> <span class="o">+</span> <span class="p">[</span><span class="n">assassin</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ours</span><span class="p">))]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theirs</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="c">#this is an SVM by default</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre>
</div>

<p>Note how we don’t want to learn a bias term, as we’re really looking for pure similarity to the hyperplane. We can also afford no regularization, as there’s not really a concept of generalization here. The resulting vector is not normalized the same way as google’s word2vec vectors are, but this won’t affect the relative differences in similarity which are important.</p>

<p>How do we find the closest word vector to the learned hyperplane? Luckily, <code class="highlighter-rouge">gensim</code> has <code class="highlighter-rouge">similar_by_vector</code>, which does exactly what we want:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">candidates</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre>
</div>

<p>Finding the words for these candidates the same way as before gives us:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[{'flashbulbs': ['star', 'limousine', 'wind', 'watch', 'day']},
 {'wattage': ['star', 'wind', 'Berlin', 'limousine', 'watch']},
 {'VIPs': ['limousine', 'watch', 'star', 'mole', 'Berlin']},
 {'radar': ['watch', 'wind', 'cap', 'star', 'mole']},
 {'invitees': ['limousine', 'star', 'watch', 'day', 'Berlin']},
 {'limos': ['limousine', 'star', 'watch', 'Berlin', 'mole']},
 {'earpieces': ['limousine', 'cap', 'watch', 'mole', 'thumb']},
 {'hotel': ['limousine', 'star', 'Berlin', 'day', 'watch']},
 {'supernovas': ['star', 'wind', 'cap', 'mole', 'day']},
 {'visor': ['cap', 'thumb', 'mole', 'watch', 'wind']}]
</code></pre>
</div>

<p>The results are very similar, which is a great sanity check. Superficially, they do also seem more “accessible” as there are fewer proper nouns. It also has the advantage of being more flexible, as the problem of over-focusing on negative examples is gone, so we can get good results for just a couple of our team’s words:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">ours_sub</span> <span class="o">=</span> <span class="p">[</span><span class="s">'watch'</span><span class="p">,</span> <span class="s">'star'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ours_sub</span> <span class="o">+</span> <span class="n">theirs</span> <span class="o">+</span> <span class="p">[</span><span class="n">assassin</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ours_sub</span><span class="p">))]</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">theirs</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre>
</div>

<p>Filtering to valid results gives a top 5 of:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>['weeknights', 'broadcasts', 'RADAR', 'viewing', 'forewarned']
</code></pre>
</div>

<p>Pretty reasonable!</p>

<p>Finally, a machine learning question: Is our projection-based approach the best way to utilize the finite hypothesis class? There might be legitimate research on this.</p>

<h2 id="extensions">Extensions</h2>

<p>Overall, we have seen that a word vector approach can be quite powerful! However, my opinion is that I can usually still come up with clues that are at least as good, with a few exceptions. The computer is certainly faster than me at it. There’s some practical and hypothetical ways to extend this! In decreasing order of practicality:</p>

<ul>
  <li>I ignored ‘bystander’ words, which don’t belong to any team, but are useful to avoid. We could introduce a weighting system for negative examples and work that into our algorithm somehow.</li>
  <li>Augment word vectors with knowledge of common colloquialisms from some corpus, and common sense relationships from WordNet and ConceptNet that are not captured by proximity-based word vectors. This will help us use clues such as “green” for “thumb” or “night” for “day” and “cap” that are obvious to people.</li>
  <li>The lesson of the game is to consider all senses of a word. We’ve used word2vec here, which just has a single vector that is meant to represent all senses. Taking an approach that allows for modeling multiple senses would likely be fruitful here.</li>
  <li>(pure hypothetical) When we play the game in real life, we know something about our teammates’ internal representations of words. One clue that was given when I played related to Doctor Who, and applied to four words! Goal: use personalized word vectors for each of your teammates :)</li>
</ul>

<p>Thanks for reading! Test out these ideas in a game and decide for yourself if word vectors are magic.</p>


</div>

<!-- disqus stuff that doesn't work -->



        </section>
        <footer>
          <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
        </footer>
      </div>
      <script src="/assets/js/scale.fix.js"></script>


    
  </body>
</html>
